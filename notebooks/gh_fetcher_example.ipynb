{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b55600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbcc394",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab9d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.modules.gh_fetcher import GithubFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884219aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = input('Enter your GH token:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e81f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghf = GithubFetcher('run-llama/llama_index', token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35449f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'OpenAI: Support new strict functionality in tool param',\n",
       "  'description': '# Description\\r\\n\\r\\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\\r\\n\\r\\nFixes # (issue)\\r\\n\\r\\n## New Package?\\r\\n\\r\\nDid I fill in the `tool.llamahub` section in the `pyproject.toml` and provide a detailed README.md for my new integration or package?\\r\\n\\r\\n- [ ] Yes\\r\\n- [ ] No\\r\\n\\r\\n## Version Bump?\\r\\n\\r\\nDid I bump the version in the `pyproject.toml` file of the package I am updating? (Except for the `llama-index-core` package)\\r\\n\\r\\n- [ ] Yes\\r\\n- [ ] No\\r\\n\\r\\n## Type of Change\\r\\n\\r\\nPlease delete options that are not relevant.\\r\\n\\r\\n- [ ] Bug fix (non-breaking change which fixes an issue)\\r\\n- [ ] New feature (non-breaking change which adds functionality)\\r\\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\\r\\n- [ ] This change requires a documentation update\\r\\n\\r\\n## How Has This Been Tested?\\r\\n\\r\\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\\r\\n\\r\\n- [ ] Added new unit/integration tests\\r\\n- [ ] Added new notebook (that tests end-to-end)\\r\\n- [ ] I stared at the code and made sure it makes sense\\r\\n\\r\\n## Suggested Checklist:\\r\\n\\r\\n- [ ] I have performed a self-review of my own code\\r\\n- [ ] I have commented my code, particularly in hard-to-understand areas\\r\\n- [ ] I have made corresponding changes to the documentation\\r\\n- [ ] I have added Google Colab support for the newly added notebooks.\\r\\n- [ ] My changes generate no new warnings\\r\\n- [ ] I have added tests that prove my fix is effective or that my feature works\\r\\n- [ ] New and existing unit tests pass locally with my changes\\r\\n- [ ] I ran `make format; make lint` to appease the lint gods\\r\\n',\n",
       "  'files': [{'filename': 'docs/docs/examples/llm/openai.ipynb',\n",
       "    'patch': '@@ -52,6 +52,18 @@\\n     \"## Basic Usage\"\\n    ]\\n   },\\n+  {\\n+   \"cell_type\": \"code\",\\n+   \"execution_count\": null,\\n+   \"id\": \"e0a6c491\",\\n+   \"metadata\": {},\\n+   \"outputs\": [],\\n+   \"source\": [\\n+    \"import os\\\\n\",\\n+    \"\\\\n\",\\n+    \"os.environ[\\\\\"OPENAI_API_KEY\\\\\"] = \\\\\"sk-...\\\\\"\"\\n+   ]\\n+  },\\n   {\\n    \"cell_type\": \"markdown\",\\n    \"id\": \"8ead155e-b8bd-46f9-ab9b-28fc009361dd\",\\n@@ -82,7 +94,7 @@\\n      \"name\": \"stdout\",\\n      \"output_type\": \"stream\",\\n      \"text\": [\\n-      \"a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his influential essays on startups and technology. Graham has also founded several successful companies, including Viaweb (which was acquired by Yahoo) and the social news website Reddit. He is considered a thought leader in the tech industry and has been a vocal advocate for startup culture and innovation.\\\\n\"\\n+      \"a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on Lisp, a programming language. Graham has also written several influential essays on startups, technology, and entrepreneurship.\\\\n\"\\n      ]\\n     }\\n    ],\\n@@ -127,7 +139,7 @@\\n      \"name\": \"stdout\",\\n      \"output_type\": \"stream\",\\n      \"text\": [\\n-      \"assistant: Ahoy matey! The name\\'s Captain Rainbowbeard, the most colorful pirate on the seven seas! What can I do for ye today? Arrr!\\\\n\"\\n+      \"assistant: Ahoy matey! The name\\'s Rainbow Roger, the most colorful pirate on the seven seas! What can I do for ye today?\\\\n\"\\n      ]\\n     }\\n    ],\\n@@ -270,9 +282,7 @@\\n      \"name\": \"stdout\",\\n      \"output_type\": \"stream\",\\n      \"text\": [\\n-      \"\\\\n\",\\n-      \"\\\\n\",\\n-      \"Paul Graham is an entrepreneur, venture capitalist, and computer scientist. He is best known for his work in the startup world, having co-founded the accelerator Y Combinator and investing in hundreds of startups. He is also a prolific writer, having authored several books on topics such as startups, programming, and technology.\\\\n\"\\n+      \"a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on Lisp, a programming language. Graham has also written several influential essays on startups, technology, and entrepreneurship.\\\\n\"\\n      ]\\n     }\\n    ],\\n@@ -306,8 +316,7 @@\\n      \"name\": \"stdout\",\\n      \"output_type\": \"stream\",\\n      \"text\": [\\n-      \"assistant: \\\\n\",\\n-      \"My name is Captain Jack Sparrow.\\\\n\"\\n+      \"assistant: Ahoy matey! The name\\'s Captain Rainbowbeard, the most colorful pirate on the seven seas! What can I do for ye today? Arrr!\\\\n\"\\n      ]\\n     }\\n    ],\\n@@ -354,6 +363,16 @@\\n     \"tool = FunctionTool.from_defaults(fn=generate_song)\"\\n    ]\\n   },\\n+  {\\n+   \"cell_type\": \"markdown\",\\n+   \"id\": \"daec99fe\",\\n+   \"metadata\": {},\\n+   \"source\": [\\n+    \"The `strict` parameter tells OpenAI whether or not to use constrained sampling when generating tool calls/structured outputs. This means that the generated tool call schema will always contain the expected fields.\\\\n\",\\n+    \"\\\\n\",\\n+    \"Since this seems to increase latency, it defaults to false.\"\\n+   ]\\n+  },\\n   {\\n    \"cell_type\": \"code\",\\n    \"execution_count\": null,\\n@@ -364,15 +383,19 @@\\n      \"name\": \"stdout\",\\n      \"output_type\": \"stream\",\\n      \"text\": [\\n-      \"name=\\'Sunshine\\' artist=\\'John Smith\\'\\\\n\"\\n+      \"name=\\'Random Vibes\\' artist=\\'DJ Chill\\'\\\\n\"\\n      ]\\n     }\\n    ],\\n    \"source\": [\\n     \"from llama_index.llms.openai import OpenAI\\\\n\",\\n     \"\\\\n\",\\n-    \"llm = OpenAI(model=\\\\\"gpt-3.5-turbo\\\\\")\\\\n\",\\n-    \"response = llm.predict_and_call([tool], \\\\\"Generate a song\\\\\")\\\\n\",\\n+    \"llm = OpenAI(model=\\\\\"gpt-4o-mini\\\\\", strict=True)\\\\n\",\\n+    \"response = llm.predict_and_call(\\\\n\",\\n+    \"    [tool],\\\\n\",\\n+    \"    \\\\\"Pick a random song for me\\\\\",\\\\n\",\\n+    \"    # strict=True  # can also be set at the function level to override the class\\\\n\",\\n+    \")\\\\n\",\\n     \"print(str(response))\"\\n    ]\\n   },'},\n",
       "   {'filename': 'llama-index-integrations/llms/llama-index-llms-openai/llama_index/llms/openai/base.py',\n",
       "    'patch': '@@ -201,6 +201,10 @@ class OpenAI(FunctionCallingLLM):\\n     api_key: str = Field(default=None, description=\"The OpenAI API key.\")\\n     api_base: str = Field(description=\"The base URL for OpenAI API.\")\\n     api_version: str = Field(description=\"The API version for OpenAI API.\")\\n+    strict: bool = Field(\\n+        default=False,\\n+        description=\"Whether to use strict mode for invoking tools/using schemas.\",\\n+    )\\n \\n     _client: Optional[SyncOpenAI] = PrivateAttr()\\n     _aclient: Optional[AsyncOpenAI] = PrivateAttr()\\n@@ -229,6 +233,7 @@ def __init__(\\n         completion_to_prompt: Optional[Callable[[str], str]] = None,\\n         pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,\\n         output_parser: Optional[BaseOutputParser] = None,\\n+        strict: bool = False,\\n         **kwargs: Any,\\n     ) -> None:\\n         additional_kwargs = additional_kwargs or {}\\n@@ -257,6 +262,7 @@ def __init__(\\n             completion_to_prompt=completion_to_prompt,\\n             pydantic_program_mode=pydantic_program_mode,\\n             output_parser=output_parser,\\n+            strict=strict,\\n             **kwargs,\\n         )\\n \\n@@ -832,6 +838,7 @@ def _prepare_chat_with_tools(\\n         verbose: bool = False,\\n         allow_parallel_tool_calls: bool = False,\\n         tool_choice: Union[str, dict] = \"auto\",\\n+        strict: Optional[bool] = None,\\n         **kwargs: Any,\\n     ) -> Dict[str, Any]:\\n         \"\"\"Predict and call the tool.\"\"\"\\n@@ -840,6 +847,20 @@ def _prepare_chat_with_tools(\\n         # misralai uses the same openai tool format\\n         tool_specs = [tool.metadata.to_openai_tool() for tool in tools]\\n \\n+        # if strict is passed in, use, else default to the class-level attribute, else default to True`\\n+        if strict is not None:\\n+            strict = strict\\n+        else:\\n+            strict = self.strict\\n+\\n+        if self.metadata.is_function_calling_model:\\n+            for tool_spec in tool_specs:\\n+                if tool_spec[\"type\"] == \"function\":\\n+                    tool_spec[\"function\"][\"strict\"] = strict\\n+                    tool_spec[\"function\"][\"parameters\"][\\n+                        \"additionalProperties\"\\n+                    ] = False  # in current openai 1.40.0 it is always false.\\n+\\n         if isinstance(user_msg, str):\\n             user_msg = ChatMessage(role=MessageRole.USER, content=user_msg)\\n '},\n",
       "   {'filename': 'llama-index-integrations/llms/llama-index-llms-openai/pyproject.toml',\n",
       "    'patch': '@@ -29,11 +29,12 @@ exclude = [\"**/BUILD\"]\\n license = \"MIT\"\\n name = \"llama-index-llms-openai\"\\n readme = \"README.md\"\\n-version = \"0.1.27\"\\n+version = \"0.1.28\"\\n \\n [tool.poetry.dependencies]\\n python = \">=3.8.1,<4.0\"\\n llama-index-core = \"^0.10.57\"\\n+openai = \"^1.40.0\"\\n \\n [tool.poetry.group.dev.dependencies]\\n ipython = \"8.10.0\"'}],\n",
       "  'commits_messages': ['OpenAI: Support new strict functionality in tool param',\n",
       "   'pr reviews',\n",
       "   'Add additional parameters false and check that it is a function calling model',\n",
       "   'nits',\n",
       "   'update notebook'],\n",
       "  'comments': [None, None, None, None],\n",
       "  'url': 'https://api.github.com/repos/run-llama/llama_index/pulls/15177'},\n",
       " {'title': 'Empty array being send to vector store',\n",
       "  'description': '# Description\\r\\n\\r\\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\\r\\n\\r\\nFixes # (issue)\\r\\n\\r\\n## New Package?\\r\\n\\r\\nDid I fill in the `tool.llamahub` section in the `pyproject.toml` and provide a detailed README.md for my new integration or package?\\r\\n\\r\\n- [ ] Yes\\r\\n- [ ] No\\r\\n\\r\\n## Version Bump?\\r\\n\\r\\nDid I bump the version in the `pyproject.toml` file of the package I am updating? (Except for the `llama-index-core` package)\\r\\n\\r\\n- [ ] Yes\\r\\n- [ ] No\\r\\n\\r\\n## Type of Change\\r\\n\\r\\nPlease delete options that are not relevant.\\r\\n\\r\\n- [ ] Bug fix (non-breaking change which fixes an issue)\\r\\n- [ ] New feature (non-breaking change which adds functionality)\\r\\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\\r\\n- [ ] This change requires a documentation update\\r\\n\\r\\n## How Has This Been Tested?\\r\\n\\r\\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\\r\\n\\r\\n- [ ] Added new unit/integration tests\\r\\n- [ ] Added new notebook (that tests end-to-end)\\r\\n- [ ] I stared at the code and made sure it makes sense\\r\\n\\r\\n## Suggested Checklist:\\r\\n\\r\\n- [ ] I have performed a self-review of my own code\\r\\n- [ ] I have commented my code, particularly in hard-to-understand areas\\r\\n- [ ] I have made corresponding changes to the documentation\\r\\n- [ ] I have added Google Colab support for the newly added notebooks.\\r\\n- [ ] My changes generate no new warnings\\r\\n- [ ] I have added tests that prove my fix is effective or that my feature works\\r\\n- [ ] New and existing unit tests pass locally with my changes\\r\\n- [ ] I ran `make format; make lint` to appease the lint gods\\r\\n',\n",
       "  'files': [{'filename': 'llama-index-core/llama_index/core/ingestion/pipeline.py',\n",
       "    'patch': '@@ -550,12 +550,13 @@ def run(\\n             )\\n \\n         if self.vector_store is not None:\\n-            self.vector_store.add([n for n in nodes if n.embedding is not None])\\n+            nodes_with_embeddings = [n for n in nodes if n.embedding is not None]\\n+            if nodes_with_embeddings:\\n+                self.vector_store.add(nodes_with_embeddings)\\n \\n         return nodes\\n \\n     # ------ async methods ------\\n-\\n     async def _ahandle_duplicates(\\n         self,\\n         nodes: List[BaseNode],\\n@@ -733,8 +734,8 @@ async def arun(\\n             )\\n \\n         if self.vector_store is not None:\\n-            await self.vector_store.async_add(\\n-                [n for n in nodes if n.embedding is not None]\\n-            )\\n+            nodes_with_embeddings = [n for n in nodes if n.embedding is not None]\\n+            if nodes_with_embeddings:\\n+                await self.vector_store.async_add(nodes_with_embeddings)\\n \\n         return nodes'}],\n",
       "  'commits_messages': ['Empty array being send to vector store'],\n",
       "  'comments': [None, None, None],\n",
       "  'url': 'https://api.github.com/repos/run-llama/llama_index/pulls/14859'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "start_date = datetime.fromisoformat(\"2024-01-01T00:00:00Z\")\n",
    "end_date = datetime.fromisoformat(\"2025-12-31T23:59:59Z\")\n",
    "\n",
    "llm_input = ghf.export_pr_data('bmax', start_date, end_date)\n",
    "llm_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
